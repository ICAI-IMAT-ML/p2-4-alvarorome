{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AYMATJ8kleZD"
   },
   "source": [
    "# Laboratory 2.4: Linear Regression contd.\n",
    "\n",
    "In this practice you will extend your Linear Regression code to a more general case. Here you will need the `synthetic_dataset.csv` present in the .zip file you downloaded alongside this notebook.\n",
    "\n",
    "In addition, we will be using the following libraries:\n",
    "- Data management:\n",
    "    - [numpy](https://numpy.org/)\n",
    "    - [pandas](https://pandas.pydata.org/)\n",
    "    - [scipy](https://scipy.org/)\n",
    "- Modelling:\n",
    "    - [scikit-learn](https://scikit-learn.org)\n",
    "- Plotting:\n",
    "    - [seaborn](https://seaborn.pydata.org/)\n",
    "    - [matplotlib](https://matplotlib.org/)\n",
    "    \n",
    "### **All the things you need to do are marked by a \"TODO\" comment nearby. Make sure you *read carefully everything before working* and solve each point before submitting your solution.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "xgMlbAFVleZE"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "# Get the absolute path of the project root\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "\n",
    "# Add it to sys.path\n",
    "sys.path.insert(0, project_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mVG3fnFnleZF"
   },
   "source": [
    "### Custom Linear Regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are going to be working with the following class that you already know from the previous practice, so feel free to recycle as much code as you want (or can). In this case, you will be enhancing its functionalities, getting a more general function than the one you implemented before. \n",
    "\n",
    "**For now, just continue with the practice and do not fill anything, you will come back later to fill the gaps.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "-CsK6Eq7leZF"
   },
   "outputs": [],
   "source": [
    "from src.Lab_2_4_LR2 import LinearRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data**: Remember, from the previous lab session, we had the following univariate dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data to use in this practice\n",
    "x = np.array([0, 3, 2, 1, 4, 6, 7, 8, 9, 10])\n",
    "y = np.array([2, 3, 2, 4, 5, 7, 9, 9, 10, 13])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the code *you wrote* from the previous practice (not the sklearn version) to fit the data in the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg = LinearRegressor()\n",
    "linreg.fit(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get into the things you'll do today, let's construct a function that provides the values for the $R^2$, RMSE and MAE. For this, the inputs of this function are the true $y$ values and the predicted $\\hat{y}$ values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.Lab_2_4_LR2 import evaluate_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain regression performance metrics\n",
    "y_pred = linreg.predict(x)\n",
    "evaluation_metrics = evaluate_regression(y, y_pred)\n",
    "print(evaluation_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous practice, you have trained a model assuming that the relationship between input and output is linear. However, in most real cases this is not common and the relationship between input and output is not linear. In this section, we are going to learn how to deal with non-linear relationships when using linear models. Read `synthetic_dataset.csv` and train a linear regression model. \n",
    "\n",
    "*The target variable is the last column of the dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Input1    Input2    Input3    Input4     Output\n",
      "0     1.764052 -0.202117  0.330046  0.371232   6.788989\n",
      "1     0.400157 -0.833231 -0.000480  0.304784   2.972974\n",
      "2     0.978738  1.733600  0.818116  0.504125   7.089140\n",
      "3     2.240893  0.190649  0.428214  0.135300  11.170032\n",
      "4     1.867558 -0.177810 -2.503947  0.653759   8.199762\n",
      "...        ...       ...       ...       ...        ...\n",
      "9995 -1.809282  0.271662 -0.400898 -1.457199   4.251336\n",
      "9996  0.042359 -0.108997 -0.585452  2.011115   4.873215\n",
      "9997  0.516872 -0.057259 -1.511276  1.689858   5.574173\n",
      "9998 -0.032921 -1.058931  0.976844  0.199966   3.842855\n",
      "9999  1.298111 -0.326528  1.255501 -1.224331   1.955963\n",
      "\n",
      "[10000 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"C:/Users/alvar/Desktop/TRABAJOS ÁLVARO UNI/2º\\MACHINE/p2-4-alvarorome/data/synthetic_dataset.csv\")\n",
    "print(data)\n",
    "# TODO: Obtain inputs and output from data\n",
    "X = data[['Input1', 'Input2', 'Input3', 'Input4']].values\n",
    "y = data['Output'].values   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we will skip the preprocessing and go straight to the modelling phase. Therefore, fit the model here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train linear regression model\n",
    "linreg2 = LinearRegressor()\n",
    "linreg2.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Obtain and print the regression performance metrics\n",
    "y_predict = linreg.predict(X)\n",
    "evaluation_metrics = evaluate_regression(y, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you trained the model well, the $R^2$ metric will be terrible, in the order of $0.5$. **What happened here?** Let's try to clear this via the *study of the residuals*.\n",
    "\n",
    "Create a function to plot the residuals of the model. This function shall:\n",
    "- Create a **histogram** of the residuals.\n",
    "- Create a **Q-Q plot** of the residuals.\n",
    "- Create a **scatterplot of the residuals against each input variable, the true output variable and the predictions**.\n",
    "\n",
    "**Why do we want to check the residuals this way?** \n",
    "\n",
    "> Write your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_residuals(data, output_column, prediction_column):\n",
    "    \"\"\"\n",
    "    Plot residuals of a model against all variables in the DataFrame, using box plots for\n",
    "    categorical variables and scatter plots for continuous variables. Additionally, plot\n",
    "    a histogram and a QQ-plot of the residuals.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): The DataFrame containing the data.\n",
    "        output_column (str): The name of the output column.\n",
    "        prediction_column (str): The name of the prediction column.\n",
    "    \"\"\"\n",
    "    # TODO: Calculate residuals\n",
    "    residuals = data[output_column].values - data[prediction_column].values\n",
    "    \n",
    "    num_features = len(data.columns) - 2  # Exclude output and prediction columns\n",
    "\n",
    "    # Determine the number of rows and columns for subplots\n",
    "    num_rows = int(np.ceil(np.sqrt(num_features + 4)))  # Add 4 for histogram, Q-Q plot, true output vs residuals, and predictions vs residuals\n",
    "    num_cols = int(np.ceil((num_features + 4) / num_rows))\n",
    "\n",
    "    # Plot histogram of residuals\n",
    "    plt.figure(figsize=(5 * num_cols, 4 * num_rows))\n",
    "    plt.subplot(num_rows, num_cols, 1)\n",
    "    plt.hist(residuals, bins=30, edgecolor='black')\n",
    "    plt.title('Histogram of Residuals')\n",
    "    plt.xlabel('Residuals')\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "    # TODO: Plot Q-Q plot of residuals (tip: use stats.probplot from scipy)\n",
    "    plt.subplot(num_rows, num_cols, 2)\n",
    "    stats.probplot(residuals, dist=\"norm\", plot=plt)       # Fill the code here\n",
    "    plt.title('Q-Q Plot of Residuals')\n",
    "\n",
    "    # TODO: Plot residuals against output variable\n",
    "    plt.subplot(num_rows, num_cols, 3)\n",
    "    plt.scatter(data[output_column], residuals, alpha=0.5)     # Fill the code here\n",
    "    plt.title('Residuals vs True Output')\n",
    "    plt.xlabel('True Output')\n",
    "    plt.ylabel('Residuals')\n",
    "\n",
    "    # Plot residuals against prediction variable\n",
    "    plt.subplot(num_rows, num_cols, 4)\n",
    "    plt.scatter(data[prediction_column], residuals, alpha=0.5)   # Use this as example for later\n",
    "    plt.title('Residuals vs Predictions')\n",
    "    plt.xlabel('Predictions')\n",
    "    plt.ylabel('Residuals')\n",
    "\n",
    "    # TODO: Plot residuals against each input variable\n",
    "    for i, col in enumerate(data.columns):\n",
    "        if col not in [output_column, prediction_column]:\n",
    "            plt.subplot(num_rows, num_cols, i + 5)\n",
    "            plt.scatter(data[col], residuals, alpha=0.5)   # Fill the code here\n",
    "            plt.title(f'Residuals vs {col}')\n",
    "            plt.xlabel(col)\n",
    "            plt.ylabel('Residuals')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, **print the coefficients** of the model **alongside the plots** you can generate with the previous function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:  First, construct a dictionary of the coefficients to print them\n",
    "coef_dict = {}\n",
    "for i, coeficient in enumerate(linreg.coefficients):\n",
    "    coef_dict[f\"x{i+1}\"] = coeficient\n",
    "print(coef_dict)\n",
    "\n",
    "# Plot the residuals for the predictions\n",
    "data[\"Predictions\"] = linreg.predict(X)\n",
    "plot_residuals(data, 'Output', 'Predictions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What are we seeing here? Is there any way to improve the model?**\n",
    "> Dadas estas gráficas, se podría decir y asumir con cierta seguridad que el modelo no es lineal por lo curvas que son dichas gráficas\n",
    "Como comentario adicional, se podría inducir como hipótesis la idea de que fuese una distribución normal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may expect, you should see some higher-order contribution to the behavior of the data, since some of the residuals have clear structures. In particular, `Input1` has a higher-order polynomial contribution (*which order do you think?*), while `Input2` has an *exponential* form. \n",
    "\n",
    "In order to fit a regression model with these contributions, construct a new dataframe where each column corresponds to the desired manipulation of each variable. Then, fit the regression model and see the results.\n",
    "\n",
    "* Also, pay attention to the fact that you can use whatever contribution you see fit here, not just these previous ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "data_transf = pd.DataFrame({'Input1': data['Input1'].values ** 2,   # Substitute \"None\" by the degree you think works here\n",
    "                            'Input2': np.exp(data['Input2'].values),\n",
    "                            'Input3': data['Input3'].values,\n",
    "                            'Input4': data['Input4'].values,\n",
    "                            'Output': data['Output'].values})\n",
    "\n",
    "X_transf = data_transf.iloc[:,:3].values\n",
    "\n",
    "# Train linear regression model\n",
    "linreg = LinearRegressor()\n",
    "linreg.fit(X_transf, y)\n",
    "\n",
    "# Evaluate the metrics to see the behavior\n",
    "y_pred = linreg.predict(X_transf)\n",
    "evaluation_metrics = evaluate_regression(y, y_pred)\n",
    "print(evaluation_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, print again the coefficients for the model and plot the residuals as you did before.\n",
    "\n",
    "**What do you observe?**\n",
    "> Al contrario que antes, el histograma es de forma más simétrica que el anterior y por ello aumenta su linealidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_dict = {'Input' + str(i+1): coef for i, coef in enumerate(linreg.coefficients)}\n",
    "print(f\"Coefficients of the model: {coef_dict}\")\n",
    "data_transf[\"Predictions\"] = linreg.predict(X_transf)\n",
    "plot_residuals(data_transf, 'Output', 'Predictions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression, as is, can not deal with categorical variables. Therefore, we need to encode the variables when preprocessing the data. Complete the one-hot-encode function below. Take into account that strings shall be treated automatically as categorical variables.\n",
    "<center>\n",
    "\n",
    "![Image](https://miro.medium.com/v2/resize:fit:1358/1*ggtP4a5YaRx6l09KQaYOnw.png)\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.Lab_2_4_LR2 import one_hot_encode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use this in a usual dataset from the internet. If everything is well, you should be able to run the following code as-is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "url = \"https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/insurance.csv\"\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "\n",
    "y = data['charges'].values\n",
    "X = data.drop(columns=['charges']).values\n",
    "\n",
    "# Preprocess the data\n",
    "# Identify categorical columns for one-hot encoding\n",
    "categorical_columns = [data.columns.get_loc(col) for col in ['sex', 'smoker', 'region']]\n",
    "\n",
    "# One-hot encode categorical variables\n",
    "X_encoded = one_hot_encode(X, categorical_columns, drop_first=True)\n",
    "X_encoded = X_encoded.astype(float)                                  # Watch out for this!\n",
    "\n",
    "# Instantiate and fit the LinearRegressor\n",
    "model = LinearRegressor()\n",
    "model.fit(X_encoded, y)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_encoded)\n",
    "evaluation_metrics = evaluate_regression(y, y_pred)\n",
    "print(evaluation_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare yourself with scikit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Load the dataset\n",
    "url = \"https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/insurance.csv\"\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "# Preprocess the data\n",
    "# TODO: One-hot encode categorical variables. Use pd.get_dummies()\n",
    "data_encoded = pd.get_dummies(data) \n",
    "\n",
    "# Split the data into features (X) and target (y)\n",
    "X = data_encoded.drop('charges', axis=1)\n",
    "y = data_encoded['charges']\n",
    "\n",
    "# Instantiate the LinearRegression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model on the training data\n",
    "model.fit(X, y)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluation_metrics = evaluate_regression(y, y_pred)\n",
    "print(evaluation_metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colored residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have categorical variables, the relationship between inputs and outputs might differ for different levels of the categorical variables. Therefore, you will modify the `plot_residuals` function to **color the scatter plots based on the value of a specific categorical variable**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_residuals(data, output_column, prediction_column, categorical_var=None):\n",
    "    \"\"\"\n",
    "    Plot residuals of a model against all variables in the DataFrame. Additionally, plot\n",
    "    a histogram and a QQ-plot of the residuals.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): The DataFrame containing the data.\n",
    "        output_column (str): The name of the output column.\n",
    "        prediction_column (str): The name of the prediction column.\n",
    "        categorical_var (str, optional): The name of a categorical variable for coloring. Defaults to None.\n",
    "    \"\"\"\n",
    "    # TODO: As before, calculate residuals\n",
    "    data['residuals'] = data[output_column].values - data[prediction_column].values\n",
    "    # Identify columns to plot (excluding the output and prediction columns)\n",
    "    columns_to_plot = [col for col in data.columns if col not in [output_column, prediction_column, 'residuals']]\n",
    "\n",
    "    # Number of rows and columns for the subplot\n",
    "    n_cols = 3\n",
    "    n_rows = int(len(columns_to_plot) / n_cols) + 2  # Additional row for histogram and QQ-plot\n",
    "\n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 5, n_rows * 5))\n",
    "\n",
    "    # Flatten the axes array for easy iteration\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # TODO: Plot each variable against the residuals\n",
    "    for i, col in enumerate(columns_to_plot):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        if categorical_var and categorical_var in data.columns:\n",
    "            sns.scatterplot(x = data[col], y = data[\"residuals\"], data=data, ax=ax hue = data[categorical_var])\n",
    "        else:\n",
    "            sns.scatterplot(x = data[col], y = data[\"residuals\"], data=data, ax=ax)\n",
    "        \n",
    "        ax.set_title(f'Residuals vs {col}')\n",
    "        ax.axhline(0, ls='--', color='r')\n",
    "\n",
    "    # Histogram of residuals\n",
    "    sns.histplot(data['residuals'], kde=True, ax=axes[i + 1])\n",
    "    axes[i + 1].set_title('Histogram of Residuals')\n",
    "\n",
    "    # QQ-plot of residuals\n",
    "    stats.probplot(data['residuals'], dist=\"norm\", plot=axes[i + 2])\n",
    "    axes[i + 2].set_title('QQ-Plot of Residuals')\n",
    "\n",
    "    # Hide any unused axes\n",
    "    for j in range(i + 3, len(axes)):\n",
    "        axes[j].set_visible(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "data['predictions'] = model.predict(X)\n",
    "# Example usage\n",
    "plot_residuals(data, 'charges', 'predictions', 'smoker')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What's happening with the residuals?**\n",
    "> Con estos datos residuales se puede obtener más información de la ya obtenida previamente y se puede confirmar su\n",
    "falta de linealidad además de confirmar la hipótesis de ser una distribución normal, en este caso, centrada en 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _Rolling in the deep_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the theory taught in class, do the following:\n",
    "\n",
    "1. **implement the gradient descent algorithm** to train the linear regression model and reproduce the results using this training method. You can find the structure for the gradient descent inside the initial linear regression function, inside the method `fit_gradient_descent`.\n",
    "2. As final steps, **display the progress of the loss function by plotting the gradient descent steps on the X-axis and the loss function on the Y-axis for each step**. \n",
    "3. Additionally, **using the same representation as in the previous section where each axis represents the values of w and b, show the sequence of steps that bring you closer to the optimum each time**. Each step should be a point in space, with coordinates (w,b). Compare all the results with the optimal solution from the scikit fit coefficients.\n",
    "\n",
    "Feel free to add as many cells as you may need from here onwards in order to fulfill these three tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg1 = LinearRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 1000000\n",
    "mse, coeficients, intercepts = linreg.fit(\n",
    "    X_encoded, y, method=\"gradient_descent\", learning_rate=0.0001, iterations=iterations\n",
    ")\n",
    "\n",
    "y_predict1 = linreg1.predict(X_encoded)\n",
    "evaluation_metrics1 = evaluate_regression(y, y_predict1)\n",
    "print(evaluation_metrics1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modelo scikit\n",
    "linreg2 = LinearRegression()\n",
    "linreg2.fit(X_encoded, y)\n",
    "\n",
    "y_predict2 = linreg2.predict(X_encoded)\n",
    "evaluation_metrics2 = evaluate_regression(y, y_predict2)\n",
    "print(evaluation_metrics2)\n",
    "\n",
    "intercept2= linreg2.intercept_\n",
    "coefficients2 = linreg2.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_space = list(range(iterations))\n",
    "\n",
    "num_columns = 10\n",
    "num_coeficients = 8\n",
    "\n",
    "fig, axes = plt.subplots(num_columns, 1, figsize=(12, 4 * num_columns))\n",
    "plt.subplots_adjust(hspace=0.3)\n",
    "\n",
    "axes[0].plot(iter_space, mse, \"-\", label=\"MSE\", color=\"green\")\n",
    "axes[0].set_title(\"Error Cuadrático Medio (MSE)\")\n",
    "axes[0].axhline(evaluation_metrics2[\"RMSE\"] ** 2, label=\"MSE de SkLearn\", color=\"black\")\n",
    "axes[0].legend()\n",
    "axes[0].grid()\n",
    "\n",
    "axes[1].plot(iter_space, intercepts, \"-\", label=\"Intercepto\", color=\"blue\")\n",
    "axes[1].axhline(intercept2, label=\"Intercepto de SkLearn\", color=\"black\")\n",
    "axes[1].set_title(\"Intercepto\")\n",
    "axes[1].legend()\n",
    "axes[1].grid()\n",
    "\n",
    "coef_values = [[coeficients[i][j] for i in range(iterations)] for j in range(num_coeficients)]\n",
    "\n",
    "for i in range(num_coeficients):\n",
    "    subplot_index = i + 2\n",
    "    coef_iter_values = coef_values[i]\n",
    "    axes[subplot_index].plot(iter_space, coef_iter_values, \"-\", label=f\"Coeficiente x{i}\")\n",
    "    axes[subplot_index].axhline(coefficients2[i], label=f\"Coeficiente x{i} de SkLearn\", color=\"black\")\n",
    "    axes[subplot_index].set_title(f\"Coeficiente x{i}\")\n",
    "    axes[subplot_index].legend()\n",
    "    axes[subplot_index].grid()\n",
    "\n",
    "# Etiqueta del eje X para el último subplot\n",
    "axes[-1].set_xlabel(\"Iteraciones\")\n",
    "\n",
    "# Mostrar los gráficos\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
